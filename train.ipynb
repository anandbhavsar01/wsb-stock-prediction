{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07dd3e30-9222-4d23-8288-47a5b0d5d64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7901c0cc-232e-4849-9688-e2db9cccfabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('dataset/text_and_label.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9df08b7a-c7ec-4fcb-914d-c7778b9e31fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_and_labels = pd.concat([file['title'], file['label']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20bdbe19-f5c8-4d5f-8d2c-22245248d03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[({0: \"It's\", 1: 'not', 2: 'about', 3: 'the', 4: 'money,', 5: \"it's\", 6: 'about', 7: 'sending', 8: 'a', 9: 'message.', 10: 'ðŸš€ðŸ’ŽðŸ™Œ'}, 'Neutral')\n",
      " ({0: 'Math', 1: 'Professor', 2: 'Scott', 3: 'Steiner', 4: 'says', 5: 'the', 6: 'numbers', 7: 'spell', 8: 'DISASTER', 9: 'for', 10: 'Gamestop', 11: 'shorts'}, 'Negative')\n",
      " ({0: 'Exit', 1: 'the', 2: 'system'}, 'Neutral') ...\n",
      " ({0: 'Ten', 1: 'Year', 2: 'Price', 3: 'Prediction', 4: 'for', 5: 'TSLA'}, 'Neutral')\n",
      " ({0: 'Daily', 1: 'Discussion', 2: 'Thread', 3: 'for', 4: 'August', 5: '02,', 6: '2021'}, 'Neutral')\n",
      " ({0: 'Fraternal', 1: 'Association', 2: 'of', 3: 'Gambling', 4: 'Gentlemen', 5: 'and', 6: 'Yacht', 7: 'Degenerates', 8: 'for', 9: 'August', 10: '02,', 11: '2021'}, 'Neutral')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "msk = np.random.rand(len(data_and_labels)) < 0.8\n",
    "train = data_and_labels[msk].to_records(index=False)\n",
    "for i in range(0,len(train)):\n",
    "    data = {}\n",
    "    title = train[i][0].split()\n",
    "    for j in range(0,len(title)):\n",
    "        data[j] = title[j]\n",
    "    train[i] = (data, train[i][1])\n",
    "test = data_and_labels[~msk].to_records(index=False)\n",
    "for i in range(0,len(test)):\n",
    "    data = {}\n",
    "    title = test[i][0].split()\n",
    "    for j in range(0,len(title)):\n",
    "        data[j] = title[j]\n",
    "    test[i] = (data, test[i][1])\n",
    "print(train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b936d3fc-aede-43eb-aafb-978a86bf62cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[preprocess::extract emojis]\n",
      "[preprocess::extract titles and body]\n",
      "[preprocess::complete]\n"
     ]
    }
   ],
   "source": [
    "from ipynb.fs.full.preprocessing import *\n",
    "dict_emojis, dict_titles, dict_body = preprocess('dataset/reddit_wsb.csv')\n",
    "#print(dict_emojis, dict_titles, dict_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baca0f8c-3ea2-4c10-b8ee-3ee42eb0d026",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "def remove_stop_words(dict_sentences):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    for i in dict_sentences.keys():\n",
    "        filtered = []\n",
    "        for word in dict_sentences[i]:\n",
    "            if word not in stop_words:\n",
    "                filtered.append(word)\n",
    "        dict_sentences[i] = filtered\n",
    "    return dict_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a43d3193-845b-4774-aa15-d8ec9893f697",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_titles = remove_stop_words(dict_titles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "eba6cd76-489b-4dc5-80f2-7e61741a868b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = []\n",
    "for i in filtered_titles.keys():\n",
    "    sentence = filtered_titles[i]\n",
    "    dict_sentence = {}\n",
    "    for j in range(0,len(sentence)):\n",
    "        dict_sentence[j] = sentence[j]\n",
    "    dataset.append( (dict_sentence, data_and_labels['label'][i] ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ed8ff417-43c9-4f9d-adc4-69b9fd74401c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24738\n"
     ]
    }
   ],
   "source": [
    "train = dataset[:int(len(dataset)*0.8)]\n",
    "test = dataset[int(len(dataset)*0.8):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "600a47b4-36da-48df-aa2e-79e70fb1d543",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = nltk.classify.DecisionTreeClassifier.train(train[:-1], entropy_cutoff=0, support_cutoff=0)\n",
    "sorted(classifier.labels())\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1002524d-ba58-4697-bc8e-5162f8b36646",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
