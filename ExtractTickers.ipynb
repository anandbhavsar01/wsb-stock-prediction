{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy \n",
    "import regex \n",
    "import emoji \n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#for ticker in tickers:\n",
    "  #  print(ticker)\n",
    "#    tickers_list.append(ticker.lower())\n",
    "    \n",
    "    \n",
    "def make_dict_title(reddit_df):\n",
    "    post_dict   = dict()\n",
    "    for i in range(len(reddit_df)):\n",
    "        ID          = reddit_df.index[i]\n",
    "        post_dict[ID]  = reddit_df.iloc[i]['title']\n",
    "    return post_dict\n",
    "\n",
    "def make_dict_body(reddit_df):\n",
    "    post_dict_body  = dict()\n",
    "    for i in range(len(reddit_df)):\n",
    "        ID          = reddit_df.index[i]\n",
    "        post_dict_body[ID]  = reddit_df.iloc[i]['body']\n",
    "    return post_dict_body\n",
    "\n",
    "def get_words(post):\n",
    "    split = post.split(' ')\n",
    "    normd  = split\n",
    "    #normd = [ww.lower() for ww in split]\n",
    "    return normd\n",
    "\n",
    "def get_split_text_from_dict(post_dict):\n",
    "    split_text_dict = dict()\n",
    "    for key in post_dict.keys():\n",
    "        normd  = get_words(post_dict[key])\n",
    "        split_text_dict[key] = normd\n",
    "    return split_text_dict\n",
    "\n",
    "def distinct_item_counter_from_list(word_list):\n",
    "    txt_counter       = dict()\n",
    "    for word in itertools.chain(*word_list):\n",
    "      #  print(word)\n",
    "        if word in list(txt_counter.keys()):\n",
    "            txt_counter[word]  = txt_counter[word] + 1\n",
    "              \n",
    "        else:\n",
    "            txt_counter[word]  = 1 \n",
    "    return txt_counter\n",
    "    \n",
    "def cross_check_txt_ticker(tickers, split_text_dict):\n",
    "    txt_list                = []\n",
    "    txt_ticker              = dict()\n",
    "    txt_ticker_location     = dict()\n",
    "    for key in split_text_dict.keys():\n",
    "        txt_ticker[key]      = set(tickers).intersection(set(split_text_dict[key]))\n",
    "        if len(txt_ticker[key])!= 0:\n",
    "            txt_list.append(txt_ticker[key])\n",
    "    ticker_counter = distinct_item_counter_from_list(txt_list)\n",
    "    # convert dictionary to df and sort\n",
    "    ticker_counter_df = pd.DataFrame.from_dict(ticker_counter,orient = 'index')\n",
    "    ticker_counter_df.columns = ['count']\n",
    "    ticker_counter_df = ticker_counter_df.sort_values(by = 'count',ascending = False).iloc[:50]\n",
    "    # more rigid method is to cross cehck data base of top most commonly used american words but \n",
    "    # due to time constraint, in our existing analysis we are just manually knock out words that are\n",
    "    # not tickers.\n",
    "    # note that the knockout_words should follow your ticker counter df\n",
    "    knockout_words   = ['A','IS','YOU','ON','FOR','ARE','DO','GO','UP','NOW','BE','GET','ALL','IT',\n",
    "                        'CAN','WE','BE','REAL','ME','LOVE','HAS','NEXT','OR','NEW','OUT','REAL','SO','BIG','UK']\n",
    "    \n",
    "    txt_ticker_list  = set(ticker_counter_df.index) ^ set(knockout_words)\n",
    "    for key in split_text_dict.keys():\n",
    "        txt_ticker[key]      = [set(split_text_dict[key]) & set(tickers)]\n",
    "        \n",
    "    for key in split_text_dict.keys():\n",
    "        txt_ticker_location[key]      = [set(split_text_dict[key]) & set(txt_ticker_list)]\n",
    "            \n",
    "    \n",
    "    return txt_ticker_list,txt_ticker_location\n",
    "\n",
    "def get_tickers_from_xlx():\n",
    "    equity_tickers   = pd.read_excel(data_path + 'tickers.xlsx', sheet_name = 'stocks',index_col = None)['Symbol']\n",
    "    crypto_tickers   = pd.read_excel(data_path + 'tickers.xlsx', sheet_name = 'crypto', index_col = None)['Symbol']\n",
    "    tickers          = np.append(equity_tickers,crypto_tickers).tolist()\n",
    "    tickers_list     = []\n",
    "    return tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# example usage\n",
    "\n",
    "#reddit_df = pd.read_csv(data_path + 'reddit_wsb.csv')\n",
    "#reddit_dict  = make_dict_title(reddit_df)\n",
    "#split_text_dict  = get_split_text_from_dict(reddit_dict)\n",
    "#txt_ticker,txt_ticker_location= cross_check_txt_ticker(tickers, split_text_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
