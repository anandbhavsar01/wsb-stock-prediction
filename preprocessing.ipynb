{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5310b771",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# Import statements go here\n",
    "import numpy as np\n",
    "import emoji\n",
    "import regex\n",
    "import pandas as pd\n",
    "\n",
    "from ipynb.fs.full.WordProcessing import get_processed_data, convert_to_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2899957",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions go here\n",
    "\n",
    "def extract_emojis(input_file):\n",
    "    emoji_list = []\n",
    "    emoji_mat = {}\n",
    "    posts = input_file.readlines()\n",
    "    for line in posts:\n",
    "        post = line[0]\n",
    "        data = regex.findall(r'\\X', post)\n",
    "        for word in data:\n",
    "            if any(char in emoji.EMOJI_DATA for char in word):\n",
    "                if word not in emoji_list:\n",
    "                    emoji_list.append(word)\n",
    "    \n",
    "    for key in emoji_list:\n",
    "        emoji_mat[key] = np.zeros(len(posts)-1)\n",
    "\n",
    "    i = 0\n",
    "    for line in posts:\n",
    "        post = line[0]\n",
    "        data = regex.findall(r'\\X', post)\n",
    "        for word in data:\n",
    "            for char in word:\n",
    "                if emoji_mat.get(char) is not None:\n",
    "                    emoji_mat[char][i] += 1\n",
    "        i += 1\n",
    "    return emoji_mat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc599aa-4d0c-486e-bb23-c297cfd4bf58",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run preprocessing code\n",
    "def preprocess():\n",
    "    f = open('dataset/reddit_wsb.csv', \"r\")\n",
    "    emoji_mat = extract_emojis(f)\n",
    "    for emoji_val in mat.keys():\n",
    "        print(emoji_val, np.sum(mat[emoji_val]))\n",
    "    return emoji_mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3b667e4-64ef-40c5-9850-1d679cda0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run preprocessing code\n",
    "def preprocess(f_in):\n",
    "    print(\"[preprocess::extract emojis]\")\n",
    "    f = open(f_in, \"r\")\n",
    "    my_dict_emoji = extract_emojis(f)\n",
    "\n",
    "    # open and process\n",
    "    print(\"[preprocess::extract titles and body]\")\n",
    "    data = pd.read_csv(f_in)\n",
    "    processed=get_processed_data(data)\n",
    "    my_dict_titles = convert_to_dict(processed,\"title\")\n",
    "    my_dict_body = convert_to_dict(processed,\"body\")\n",
    "    print(\"[preprocess::complete]\")\n",
    "    return my_dict_emoji, my_dict_titles, my_dict_body\n",
    "    \n",
    "#dict_emojis, dict_titles, dict_body = preprocess('dataset/reddit_wsb.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfc5a604",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# sanity check\n",
    "#dict_titles[2345]\n",
    "#dict_body[2345]\n",
    "#for emoji_val in dict_emojis.keys():\n",
    "#        print(emoji_val, np.sum(dict_emojis[emoji_val]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1bb54f0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "def get_ticker_data(f_in, tickers_found):\n",
    "    file = pd.read_csv(f_in)\n",
    "    tickers_file = pd.read_csv(tickers_found)\n",
    "    tickers_set = set()\n",
    "    for i, row in tickers_file.iterrows():\n",
    "        #print(row)\n",
    "        if not pd.isnull(row['Ticker']):\n",
    "            row_ticker_list = row['Ticker'].replace('{','').replace('}','').replace('\\'', '').strip().split(',')\n",
    "            #print(row_ticker_list)\n",
    "            for ticker in row_ticker_list:\n",
    "                tickers_set.add(ticker.strip())\n",
    "            \n",
    "    #print(tickers_set)\n",
    "    \n",
    "    data_tickers = pd.concat([file, tickers_file['Ticker']], axis=1)\n",
    "    condensed_file = data_tickers.dropna(subset=['Ticker'])\n",
    "    sorted_file = condensed_file.sort_values(['timestamp'],ascending=True)\n",
    "    start_date = sorted_file.iloc[0]['timestamp'].split()[0]\n",
    "    end_date = sorted_file.iloc[-1]['timestamp'].split()[0]\n",
    "    #print(start_date,end_date)\n",
    "    \n",
    "    ticker_str = ' '.join(tickers_set)\n",
    "    #all_tickers = yf.Tickers(ticker_str)\n",
    "    data = yf.download(ticker_str, start=start_date, end=end_date, group_by = 'ticker', auto_adjust=True, prepost=True)\n",
    "    return data, tickers_set\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b72f4984-3c56-4209-9b81-6f9df2c75035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  25 of 25 completed\n",
      "{'PLTR', 'AMD', 'SPCE', 'RKT', 'REAL', 'AMC', 'DD', 'ONE', 'GME', 'TSLA', 'CRSR', 'BB', 'RH', 'CLOV', 'AM', 'NOK', 'WISH', 'BY', 'UWMC', 'MVIS', 'APP', 'NIO', 'TD', 'SNDL', 'AAL'}\n"
     ]
    }
   ],
   "source": [
    "data, ticker_set = get_ticker_data('dataset/reddit_wsb.csv','dataset/ticker_location.csv')\n",
    "data.to_excel(\"tickerdata.xlsx\")\n",
    "print(ticker_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9f85a8ce-d82f-41f1-857f-64f5eb2387cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Date\n",
      "2021-01-28    71.399994\n",
      "2021-01-29    54.709991\n",
      "2021-02-01    91.559998\n",
      "2021-02-02    50.759995\n",
      "2021-02-03    19.599998\n",
      "                ...    \n",
      "2021-08-09    -9.330002\n",
      "2021-08-10     2.309998\n",
      "2021-08-11    -0.350006\n",
      "2021-08-12    -2.470001\n",
      "2021-08-13    -2.050003\n",
      "Length: 138, dtype: float64\n",
      "                  Open        High         Low       Close    Volume\n",
      "Date                                                                \n",
      "2021-01-28  265.000000  483.000000  112.250000  193.600006  58815800\n",
      "2021-01-29  379.709991  413.980011  250.000000  325.000000  50566100\n",
      "2021-02-01  316.559998  322.000000  212.000000  225.000000  37382200\n",
      "2021-02-02  140.759995  158.000000   74.220001   90.000000  78183100\n",
      "2021-02-03  112.010002  113.400002   85.250000   92.410004  42698500\n",
      "...                ...         ...         ...         ...       ...\n",
      "2021-08-09  151.800003  164.710007  150.660004  161.130005   2249200\n",
      "2021-08-10  161.360001  166.899994  155.350006  159.050003   1623300\n",
      "2021-08-11  158.429993  159.050003  154.619995  158.779999    945400\n",
      "2021-08-12  159.880005  164.279999  157.330002  162.350006   1317800\n",
      "2021-08-13  160.470001  163.550003  157.410004  162.520004   1014800\n",
      "\n",
      "[138 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "print(data['GME']['Open'] - data['GME']['Close'])\n",
    "print(data['GME'])\n",
    "#for line in data['GME','Open']:\n",
    "#    print(line)\n",
    "    \n",
    "#print(data['GME']['2021-01-28'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d0977c67-56db-478d-a84f-c059f30f1b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dateutil import parser\n",
    "\n",
    "def append_ticker_data(data, ticker_set):\n",
    "    file = pd.read_csv('dataset/reddit_wsb.csv')\n",
    "    tickers_file = pd.read_csv('dataset/ticker_location.csv')\n",
    "    data_tickers = pd.concat([file, tickers_file['Ticker']], axis=1)\n",
    "    condensed_file = data_tickers.dropna(subset=['Ticker'])\n",
    "    sorted_file = condensed_file.sort_values(['timestamp'],ascending=True)\n",
    "    start_date_str = sorted_file.iloc[0]['timestamp'].split()[0]\n",
    "    start_date = parser.parse(start_date_str)\n",
    "    # Store the change in stock prices for each day\n",
    "    ticker_deltas = {}\n",
    "    for ticker in ticker_set:\n",
    "        ticker_deltas[ticker] = data[ticker, 'Close'] - data[ticker, 'Open']\n",
    "    \n",
    "    print(ticker_deltas['GME'][5])\n",
    "    # Line up those changing stock prices with the dataset\n",
    "    dataset_deltas = []\n",
    "    for i, row in tickers_file.iterrows():\n",
    "        if not pd.isnull(row['Ticker']):\n",
    "            row_ticker_list = row['Ticker'].replace('{','').replace('}','').replace('\\'', '').strip().split(',')\n",
    "            date_str = file.iloc[i]['timestamp'].split()[0] # Rows correspond to each other\n",
    "            date = parser.parse(date_str)\n",
    "            \n",
    "            # Get the date difference\n",
    "            str_diff = str(date - start_date)\n",
    "            datediff = 0\n",
    "            if str_diff[0] == '0':\n",
    "                datediff = 0\n",
    "            else:\n",
    "                datediff = int(str_diff.split()[0])\n",
    "            ticker_dict = {}\n",
    "            for ticker in row_ticker_list:\n",
    "                check_ticker = ticker.strip()\n",
    "                ticker_dict[check_ticker] = ticker_deltas[check_ticker][datediff]\n",
    "            dataset_deltas.append(ticker_dict)\n",
    "        else:\n",
    "            dataset_deltas.append(None)\n",
    "    print(dataset_deltas)\n",
    "            \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ff464a52-fb09-4028-8e86-0f56945a6ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-37.69000244140625\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 143 is out of bounds for axis 0 with size 138",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mappend_ticker_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mticker_set\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [68]\u001b[0m, in \u001b[0;36mappend_ticker_data\u001b[0;34m(data, ticker_set)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m row_ticker_list:\n\u001b[1;32m     34\u001b[0m         check_ticker \u001b[38;5;241m=\u001b[39m ticker\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[0;32m---> 35\u001b[0m         ticker_dict[check_ticker] \u001b[38;5;241m=\u001b[39m \u001b[43mticker_deltas\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcheck_ticker\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mdatediff\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     36\u001b[0m     dataset_deltas\u001b[38;5;241m.\u001b[39mappend(ticker_dict)\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/mnt/c/Users/anand/Documents/CS229/project/wsb-stock-prediction/env/lib/python3.8/site-packages/pandas/core/series.py:955\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    952\u001b[0m     key \u001b[38;5;241m=\u001b[39m unpack_1tuple(key)\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(key) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_should_fallback_to_positional:\n\u001b[0;32m--> 955\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_values\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    957\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m    958\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_value(key)\n",
      "\u001b[0;31mIndexError\u001b[0m: index 143 is out of bounds for axis 0 with size 138"
     ]
    }
   ],
   "source": [
    "append_ticker_data(data, ticker_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933f6810-4ea9-47a6-8910-9b17c2718fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
